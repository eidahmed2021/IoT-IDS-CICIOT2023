{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìÖ Day 6: Robustness Analysis + Adversarial Training\n",
                "## Noise Perturbation, Feature Perturbation, Adversarial Retraining ‚Äî GPU\n",
                "\n",
                "---\n",
                "\n",
                "**Steps:**\n",
                "1. Load best models from Day 3-5\n",
                "2. Gaussian noise perturbation test (0.01, 0.05, 0.1, 0.2, 0.5)\n",
                "3. Feature perturbation (adversarial evasion simulation)\n",
                "4. Adversarial retraining (add noisy samples)\n",
                "5. Before vs After comparison\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.add_dll_directory(r'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v13.1\\bin\\x64')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import xgboost as xgb\n",
                "from sklearn.metrics import accuracy_score, f1_score\n",
                "from sklearn.utils.class_weight import compute_sample_weight\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import time\n",
                "import gc\n",
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "plt.style.use('dark_background')\n",
                "plt.rcParams['figure.figsize'] = (14, 6)\n",
                "plt.rcParams['font.size'] = 12\n",
                "\n",
                "os.makedirs('figures', exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Ready | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "print(\"üì• Loading data...\")\n",
                "X_train = np.load('processed/X_train.npy')\n",
                "X_test = np.load('processed/X_test.npy')\n",
                "\n",
                "y_binary_train = np.load('processed/y_binary_train.npy')\n",
                "y_binary_test = np.load('processed/y_binary_test.npy')\n",
                "y_family_train = np.load('processed/y_family_train.npy')\n",
                "y_family_test = np.load('processed/y_family_test.npy')\n",
                "y_subtype_train = np.load('processed/y_subtype_train.npy')\n",
                "y_subtype_test = np.load('processed/y_subtype_test.npy')\n",
                "\n",
                "with open('processed/preprocessing_metadata.json', 'r') as f:\n",
                "    meta = json.load(f)\n",
                "feature_names = meta['feature_names']\n",
                "\n",
                "# Load best models\n",
                "print(\"üì¶ Loading trained models...\")\n",
                "bst_binary = xgb.Booster()\n",
                "bst_binary.load_model('models/binary_xgb_gpu.json')\n",
                "\n",
                "bst_family = xgb.Booster()\n",
                "bst_family.load_model('models/family_best_xgb_gpu.json')\n",
                "\n",
                "bst_subtype = xgb.Booster()\n",
                "bst_subtype.load_model('models/subtype_xgb_gpu.json')\n",
                "\n",
                "print(\"‚úÖ All loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_model(bst, X, y, level='binary'):\n",
                "    \"\"\"Evaluate a model and return F1 scores.\"\"\"\n",
                "    dmat = xgb.DMatrix(X, feature_names=feature_names)\n",
                "    y_prob = bst.predict(dmat)\n",
                "    \n",
                "    if level == 'binary':\n",
                "        y_pred = (y_prob > 0.5).astype(int)\n",
                "        f1_mac = f1_score(y, y_pred, average='macro')\n",
                "    else:\n",
                "        y_pred = y_prob.argmax(axis=1)\n",
                "        f1_mac = f1_score(y, y_pred, average='macro')\n",
                "    \n",
                "    acc = accuracy_score(y, y_pred)\n",
                "    return acc, f1_mac"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîä Step 1: Gaussian Noise Perturbation Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "noise_levels = [0.0, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
                "levels_config = [\n",
                "    ('Binary', bst_binary, y_binary_test, 'binary'),\n",
                "    ('8-Class', bst_family, y_family_test, 'multi'),\n",
                "    ('34-Class', bst_subtype, y_subtype_test, 'multi')\n",
                "]\n",
                "\n",
                "noise_results = {level_name: {'accuracy': [], 'f1_macro': []} for level_name, _, _, _ in levels_config}\n",
                "\n",
                "print(\"üîä Testing noise robustness...\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "for noise in noise_levels:\n",
                "    print(f\"\\n   Noise level: {noise}\")\n",
                "    \n",
                "    if noise == 0:\n",
                "        X_noisy = X_test\n",
                "    else:\n",
                "        np.random.seed(42)\n",
                "        noise_matrix = np.random.normal(0, noise, X_test.shape).astype(np.float32)\n",
                "        X_noisy = X_test + noise_matrix\n",
                "        del noise_matrix\n",
                "    \n",
                "    for level_name, bst, y, level_type in levels_config:\n",
                "        acc, f1_mac = evaluate_model(bst, X_noisy, y, level_type)\n",
                "        noise_results[level_name]['accuracy'].append(acc)\n",
                "        noise_results[level_name]['f1_macro'].append(f1_mac)\n",
                "        print(f\"      {level_name:<10s}: Acc={acc*100:.2f}% | F1-Macro={f1_mac*100:.2f}%\")\n",
                "    \n",
                "    if noise != 0:\n",
                "        del X_noisy\n",
                "        gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìä Plot Noise vs F1\n",
                "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
                "\n",
                "colors = ['#00D4AA', '#FF6B6B', '#45B7D1']\n",
                "markers = ['o', 's', '^']\n",
                "\n",
                "for ax, metric_name in zip(axes, ['accuracy', 'f1_macro']):\n",
                "    for i, (level_name, _, _, _) in enumerate(levels_config):\n",
                "        vals = [v * 100 for v in noise_results[level_name][metric_name]]\n",
                "        ax.plot(noise_levels, vals, color=colors[i], marker=markers[i],\n",
                "                linewidth=2.5, markersize=10, label=level_name)\n",
                "    \n",
                "    ax.set_xlabel('Noise Level (œÉ)', fontsize=13)\n",
                "    ax.set_ylabel(f'{metric_name.replace(\"_\", \" \").title()} (%)', fontsize=13)\n",
                "    ax.set_title(f'{metric_name.replace(\"_\", \" \").title()} vs Noise Level', fontsize=14, fontweight='bold', color='white')\n",
                "    ax.legend(fontsize=12)\n",
                "    ax.grid(True, alpha=0.2)\n",
                "\n",
                "plt.suptitle('üîä Robustness Test ‚Äî Gaussian Noise Perturbation', fontsize=16, fontweight='bold', color='#00D4AA', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/robustness_noise_test.png', dpi=150, bbox_inches='tight', facecolor='#1a1a2e')\n",
                "plt.show()\n",
                "print(\"üíæ Saved to figures/robustness_noise_test.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Step 2: Feature Perturbation (Adversarial Evasion)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get top-5 features from the binary model\n",
                "importance = bst_binary.get_score(importance_type='weight')\n",
                "sorted_imp = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:5]\n",
                "\n",
                "top5_indices = []\n",
                "top5_names = []\n",
                "for fname, score in sorted_imp:\n",
                "    if fname.startswith('f'):\n",
                "        try:\n",
                "            idx = int(fname[1:])\n",
                "            top5_indices.append(idx)\n",
                "            top5_names.append(feature_names[idx] if idx < len(feature_names) else fname)\n",
                "        except ValueError:\n",
                "            pass\n",
                "    elif fname in feature_names:\n",
                "        idx = feature_names.index(fname)\n",
                "        top5_indices.append(idx)\n",
                "        top5_names.append(fname)\n",
                "\n",
                "print(f\"üéØ Top-5 features for perturbation:\")\n",
                "for i, (name, idx) in enumerate(zip(top5_names, top5_indices)):\n",
                "    print(f\"   {i+1}. {name} (index {idx})\")\n",
                "\n",
                "# Test perturbation of only top-5 features\n",
                "perturbation_levels = [0.0, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
                "feature_perturb_results = {'accuracy': [], 'f1_macro': []}\n",
                "\n",
                "print(f\"\\nüîä Perturbing top-5 features only:\")\n",
                "for noise in perturbation_levels:\n",
                "    X_perturbed = X_test.copy()\n",
                "    if noise > 0:\n",
                "        np.random.seed(42)\n",
                "        for idx in top5_indices:\n",
                "            X_perturbed[:, idx] += np.random.normal(0, noise, X_test.shape[0]).astype(np.float32)\n",
                "    \n",
                "    acc, f1_mac = evaluate_model(bst_binary, X_perturbed, y_binary_test, 'binary')\n",
                "    feature_perturb_results['accuracy'].append(acc)\n",
                "    feature_perturb_results['f1_macro'].append(f1_mac)\n",
                "    print(f\"   œÉ={noise:.2f}: Acc={acc*100:.2f}% | F1={f1_mac*100:.2f}%\")\n",
                "    del X_perturbed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìä Plot Feature Perturbation\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "ax.plot(perturbation_levels, [v*100 for v in feature_perturb_results['f1_macro']],\n",
                "        color='#FF4C61', marker='o', linewidth=2.5, markersize=10, label='Top-5 Features Only')\n",
                "\n",
                "# Compare with full noise from earlier\n",
                "full_noise_levels = [0.0, 0.05, 0.1, 0.2, 0.5]\n",
                "full_noise_f1 = [v*100 for v in noise_results['Binary']['f1_macro'][:-1]]  # exclude 0.5 if missing\n",
                "ax.plot(full_noise_levels[:len(full_noise_f1)], full_noise_f1,\n",
                "        color='#00D4AA', marker='s', linewidth=2.5, markersize=10, label='All Features', linestyle='--')\n",
                "\n",
                "ax.set_xlabel('Noise Level (œÉ)', fontsize=13)\n",
                "ax.set_ylabel('F1-Macro (%)', fontsize=13)\n",
                "ax.set_title('üéØ Adversarial Evasion: Top-5 Feature Perturbation vs All Features', fontsize=14, fontweight='bold', color='#00D4AA')\n",
                "ax.legend(fontsize=12)\n",
                "ax.grid(True, alpha=0.2)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/robustness_feature_perturbation.png', dpi=150, bbox_inches='tight', facecolor='#1a1a2e')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üõ°Ô∏è Step 3: Adversarial Retraining"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate noisy training data (noise=0.1)\n",
                "print(\"üõ°Ô∏è Adversarial Retraining ‚Äî Adding noisy samples to training data...\")\n",
                "np.random.seed(42)\n",
                "noise_level = 0.1\n",
                "X_train_noisy = X_train + np.random.normal(0, noise_level, X_train.shape).astype(np.float32)\n",
                "\n",
                "# Combine clean + noisy\n",
                "X_train_combined = np.vstack([X_train, X_train_noisy])\n",
                "y_binary_combined = np.concatenate([y_binary_train, y_binary_train])\n",
                "y_family_combined = np.concatenate([y_family_train, y_family_train])\n",
                "y_subtype_combined = np.concatenate([y_subtype_train, y_subtype_train])\n",
                "\n",
                "del X_train_noisy\n",
                "gc.collect()\n",
                "\n",
                "print(f\"   Original: {X_train.shape[0]:,} | Combined: {X_train_combined.shape[0]:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Retrain Binary model on GPU\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"üéÆ Adversarial Retraining ‚Äî Binary (GPU)\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "n_benign = (y_binary_combined == 0).sum()\n",
                "n_attack = (y_binary_combined == 1).sum()\n",
                "spw = n_benign / n_attack if n_attack > 0 else 1.0\n",
                "\n",
                "dtrain_adv = xgb.DMatrix(X_train_combined, label=y_binary_combined, feature_names=feature_names)\n",
                "dtest_bin = xgb.DMatrix(X_test, label=y_binary_test, feature_names=feature_names)\n",
                "\n",
                "xgb_params_bin = {\n",
                "    'tree_method': 'hist', 'device': 'cuda', 'objective': 'binary:logistic',\n",
                "    'eval_metric': ['logloss', 'error'], 'max_depth': 8, 'learning_rate': 0.1,\n",
                "    'scale_pos_weight': spw, 'subsample': 0.8, 'colsample_bytree': 0.8,\n",
                "    'verbosity': 1, 'seed': 42\n",
                "}\n",
                "\n",
                "t0 = time.time()\n",
                "bst_adv_binary = xgb.train(\n",
                "    xgb_params_bin, dtrain_adv,\n",
                "    num_boost_round=300,\n",
                "    evals=[(dtest_bin, 'test')],\n",
                "    early_stopping_rounds=20,\n",
                "    verbose_eval=50\n",
                ")\n",
                "print(f\"   üéÆ GPU | ‚è±Ô∏è {time.time()-t0:.1f}s\")\n",
                "\n",
                "bst_adv_binary.save_model('models/binary_xgb_adversarial.json')\n",
                "del dtrain_adv; gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare before vs after adversarial training\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üìä BEFORE vs AFTER Adversarial Training\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "comparison_noise = [0.0, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
                "before_f1 = []\n",
                "after_f1 = []\n",
                "\n",
                "for noise in comparison_noise:\n",
                "    if noise == 0:\n",
                "        X_noisy = X_test\n",
                "    else:\n",
                "        np.random.seed(42)\n",
                "        X_noisy = X_test + np.random.normal(0, noise, X_test.shape).astype(np.float32)\n",
                "    \n",
                "    _, f1_before = evaluate_model(bst_binary, X_noisy, y_binary_test, 'binary')\n",
                "    _, f1_after = evaluate_model(bst_adv_binary, X_noisy, y_binary_test, 'binary')\n",
                "    \n",
                "    before_f1.append(f1_before)\n",
                "    after_f1.append(f1_after)\n",
                "    \n",
                "    improvement = (f1_after - f1_before) * 100\n",
                "    print(f\"   œÉ={noise:.2f}: Before={f1_before*100:.2f}% | After={f1_after*100:.2f}% | Œî={improvement:+.2f}%\")\n",
                "    \n",
                "    if noise != 0:\n",
                "        del X_noisy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìä Before vs After plot\n",
                "fig, ax = plt.subplots(figsize=(12, 7))\n",
                "\n",
                "ax.plot(comparison_noise, [v*100 for v in before_f1], color='#FF4C61', marker='o',\n",
                "        linewidth=2.5, markersize=10, label='Before Adv. Training')\n",
                "ax.plot(comparison_noise, [v*100 for v in after_f1], color='#00D4AA', marker='s',\n",
                "        linewidth=2.5, markersize=10, label='After Adv. Training')\n",
                "\n",
                "# Fill the improvement area\n",
                "ax.fill_between(comparison_noise, [v*100 for v in before_f1], [v*100 for v in after_f1],\n",
                "                alpha=0.2, color='#00D4AA', label='Improvement')\n",
                "\n",
                "ax.set_xlabel('Noise Level (œÉ)', fontsize=13)\n",
                "ax.set_ylabel('F1-Macro (%)', fontsize=13)\n",
                "ax.set_title('üõ°Ô∏è Adversarial Retraining ‚Äî Before vs After', fontsize=16, fontweight='bold', color='#00D4AA')\n",
                "ax.legend(fontsize=12)\n",
                "ax.grid(True, alpha=0.2)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/robustness_adversarial_comparison.png', dpi=150, bbox_inches='tight', facecolor='#1a1a2e')\n",
                "plt.show()\n",
                "print(\"üíæ Saved to figures/robustness_adversarial_comparison.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìä Robustness Results Table\n",
                "robustness_table = pd.DataFrame({\n",
                "    'Noise Level': comparison_noise,\n",
                "    'Before F1 (%)': [f*100 for f in before_f1],\n",
                "    'After F1 (%)': [f*100 for f in after_f1],\n",
                "    'Improvement': [(a-b)*100 for a, b in zip(after_f1, before_f1)]\n",
                "})\n",
                "print(robustness_table.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
                "\n",
                "# Save\n",
                "robust_results = {\n",
                "    'timestamp': datetime.now().isoformat(),\n",
                "    'device': 'GPU (CUDA)',\n",
                "    'noise_test': noise_results,\n",
                "    'feature_perturbation': feature_perturb_results,\n",
                "    'adversarial_comparison': {\n",
                "        'noise_levels': comparison_noise,\n",
                "        'before_f1': before_f1,\n",
                "        'after_f1': after_f1\n",
                "    }\n",
                "}\n",
                "with open('models/robustness_results.json', 'w') as f:\n",
                "    json.dump(robust_results, f, indent=2, default=str)\n",
                "\n",
                "del X_train_combined, y_binary_combined, y_family_combined, y_subtype_combined\n",
                "gc.collect()\n",
                "\n",
                "print(\"\\nüèÜ\" * 20)\n",
                "print(f\"  ‚úÖ ROBUSTNESS ANALYSIS COMPLETE!\")\n",
                "print(\"üèÜ\" * 20)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}