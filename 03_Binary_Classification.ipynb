{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìÖ Day 3: Level 1 ‚Äî Binary Classification (Benign vs Attack)\n",
                "## 4 Models: Decision Tree, Random Forest, XGBoost GPU, LightGBM GPU\n",
                "\n",
                "---\n",
                "\n",
                "**Steps:**\n",
                "1. Load preprocessed data\n",
                "2. Train 4 models (DT, RF, XGBoost GPU, LightGBM GPU)\n",
                "3. Evaluate: Accuracy, F1, Precision, Recall, ROC-AUC, Confusion Matrix\n",
                "4. Compare all models\n",
                "5. ROC Curves\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.add_dll_directory(r'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v13.1\\bin\\x64')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import xgboost as xgb\n",
                "import lightgbm as lgb\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,\n",
                "                             roc_auc_score, classification_report, confusion_matrix,\n",
                "                             roc_curve, auc)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import time\n",
                "import gc\n",
                "import json\n",
                "import joblib\n",
                "from datetime import datetime\n",
                "\n",
                "plt.style.use('dark_background')\n",
                "plt.rcParams['figure.figsize'] = (14, 6)\n",
                "plt.rcParams['font.size'] = 12\n",
                "\n",
                "os.makedirs('models', exist_ok=True)\n",
                "os.makedirs('figures', exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Ready | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
                "print(f\"üîß XGBoost: {xgb.__version__} | LightGBM: {lgb.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì• Load Preprocessed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üì• Loading preprocessed data...\")\n",
                "t0 = time.time()\n",
                "\n",
                "X_train = np.load('processed/X_train.npy')\n",
                "X_test = np.load('processed/X_test.npy')\n",
                "y_train = np.load('processed/y_binary_train.npy')\n",
                "y_test = np.load('processed/y_binary_test.npy')\n",
                "\n",
                "with open('processed/preprocessing_metadata.json', 'r') as f:\n",
                "    meta = json.load(f)\n",
                "feature_names = meta['feature_names']\n",
                "\n",
                "# Class balance\n",
                "n_benign_train = (y_train == 0).sum()\n",
                "n_attack_train = (y_train == 1).sum()\n",
                "scale_pos_weight = n_benign_train / n_attack_train if n_attack_train > 0 else 1.0\n",
                "\n",
                "print(f\"‚úÖ Loaded in {time.time()-t0:.1f}s\")\n",
                "print(f\"   X_train: {X_train.shape} | X_test: {X_test.shape}\")\n",
                "print(f\"   Train ‚Äî Benign: {n_benign_train:,} | Attack: {n_attack_train:,}\")\n",
                "print(f\"   scale_pos_weight: {scale_pos_weight:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèãÔ∏è Train 4 Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Storage for results\n",
                "results = {}\n",
                "models = {}\n",
                "predictions = {}\n",
                "probabilities = {}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Model 1: Decision Tree\n",
                "# ============================================================\n",
                "print(\"=\"*60)\n",
                "print(\"üå≥ Model 1: Decision Tree\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "dt = DecisionTreeClassifier(\n",
                "    class_weight='balanced',\n",
                "    max_depth=15,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "t0 = time.time()\n",
                "dt.fit(X_train, y_train)\n",
                "train_time_dt = time.time() - t0\n",
                "\n",
                "t0 = time.time()\n",
                "y_pred_dt = dt.predict(X_test)\n",
                "y_prob_dt = dt.predict_proba(X_test)[:, 1]\n",
                "infer_time_dt = time.time() - t0\n",
                "\n",
                "models['Decision Tree'] = dt\n",
                "predictions['Decision Tree'] = y_pred_dt\n",
                "probabilities['Decision Tree'] = y_prob_dt\n",
                "\n",
                "print(f\"   ‚è±Ô∏è Train: {train_time_dt:.1f}s | Inference: {infer_time_dt:.1f}s\")\n",
                "print(f\"   ‚úÖ Accuracy: {accuracy_score(y_test, y_pred_dt)*100:.4f}%\")\n",
                "print(f\"   üéØ F1: {f1_score(y_test, y_pred_dt)*100:.4f}%\")\n",
                "\n",
                "joblib.dump(dt, 'models/binary_dt.joblib')\n",
                "print(\"   üíæ Saved to models/binary_dt.joblib\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Model 2: Random Forest\n",
                "# ============================================================\n",
                "print(\"=\"*60)\n",
                "print(\"üå≤ Model 2: Random Forest\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "rf = RandomForestClassifier(\n",
                "    n_estimators=200,\n",
                "    class_weight='balanced',\n",
                "    max_depth=15,\n",
                "    n_jobs=-1,\n",
                "    random_state=42,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "t0 = time.time()\n",
                "rf.fit(X_train, y_train)\n",
                "train_time_rf = time.time() - t0\n",
                "\n",
                "t0 = time.time()\n",
                "y_pred_rf = rf.predict(X_test)\n",
                "y_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
                "infer_time_rf = time.time() - t0\n",
                "\n",
                "models['Random Forest'] = rf\n",
                "predictions['Random Forest'] = y_pred_rf\n",
                "probabilities['Random Forest'] = y_prob_rf\n",
                "\n",
                "print(f\"   ‚è±Ô∏è Train: {train_time_rf:.1f}s | Inference: {infer_time_rf:.1f}s\")\n",
                "print(f\"   ‚úÖ Accuracy: {accuracy_score(y_test, y_pred_rf)*100:.4f}%\")\n",
                "print(f\"   üéØ F1: {f1_score(y_test, y_pred_rf)*100:.4f}%\")\n",
                "\n",
                "joblib.dump(rf, 'models/binary_rf.joblib')\n",
                "print(\"   üíæ Saved to models/binary_rf.joblib\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Model 3: XGBoost GPU üéÆ\n",
                "# ============================================================\n",
                "print(\"=\"*60)\n",
                "print(\"üéÆ Model 3: XGBoost GPU (CUDA)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_names)\n",
                "dtest = xgb.DMatrix(X_test, label=y_test, feature_names=feature_names)\n",
                "\n",
                "xgb_params = {\n",
                "    'tree_method': 'hist',\n",
                "    'device': 'cuda',            # üéÆ GPU\n",
                "    'objective': 'binary:logistic',\n",
                "    'eval_metric': ['logloss', 'auc', 'error'],\n",
                "    'max_depth': 8,\n",
                "    'learning_rate': 0.1,\n",
                "    'min_child_weight': 5,\n",
                "    'subsample': 0.8,\n",
                "    'colsample_bytree': 0.8,\n",
                "    'reg_alpha': 0.1,\n",
                "    'reg_lambda': 1.0,\n",
                "    'scale_pos_weight': scale_pos_weight,\n",
                "    'verbosity': 1,\n",
                "    'seed': 42\n",
                "}\n",
                "\n",
                "evals_result_xgb = {}\n",
                "t0 = time.time()\n",
                "bst_xgb = xgb.train(\n",
                "    xgb_params, dtrain,\n",
                "    num_boost_round=300,\n",
                "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
                "    early_stopping_rounds=20,\n",
                "    evals_result=evals_result_xgb,\n",
                "    verbose_eval=50\n",
                ")\n",
                "train_time_xgb = time.time() - t0\n",
                "\n",
                "t0 = time.time()\n",
                "y_prob_xgb = bst_xgb.predict(dtest, iteration_range=(0, bst_xgb.best_iteration + 1))\n",
                "y_pred_xgb = (y_prob_xgb > 0.5).astype(int)\n",
                "infer_time_xgb = time.time() - t0\n",
                "\n",
                "models['XGBoost GPU'] = bst_xgb\n",
                "predictions['XGBoost GPU'] = y_pred_xgb\n",
                "probabilities['XGBoost GPU'] = y_prob_xgb\n",
                "\n",
                "print(f\"\\n   üéÆ Trained on GPU (CUDA)\")\n",
                "print(f\"   ‚è±Ô∏è Train: {train_time_xgb:.1f}s | Inference: {infer_time_xgb:.1f}s\")\n",
                "print(f\"   üèÜ Best iteration: {bst_xgb.best_iteration}\")\n",
                "print(f\"   ‚úÖ Accuracy: {accuracy_score(y_test, y_pred_xgb)*100:.4f}%\")\n",
                "print(f\"   üéØ F1: {f1_score(y_test, y_pred_xgb)*100:.4f}%\")\n",
                "\n",
                "bst_xgb.save_model('models/binary_xgb_gpu.json')\n",
                "print(\"   üíæ Saved to models/binary_xgb_gpu.json\")\n",
                "\n",
                "del dtrain\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Model 4: LightGBM GPU üéÆ\n",
                "# ============================================================\n",
                "print(\"=\"*60)\n",
                "print(\"üéÆ Model 4: LightGBM GPU\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "lgb_train = lgb.Dataset(X_train, label=y_train, feature_name=feature_names, free_raw_data=False)\n",
                "lgb_test = lgb.Dataset(X_test, label=y_test, feature_name=feature_names, reference=lgb_train, free_raw_data=False)\n",
                "\n",
                "lgb_params = {\n",
                "    'objective': 'binary',\n",
                "    'metric': ['binary_logloss', 'auc', 'binary_error'],\n",
                "    'device': 'gpu',             # üéÆ GPU\n",
                "    'gpu_use_dp': False,\n",
                "    'is_unbalance': True,\n",
                "    'max_depth': 8,\n",
                "    'learning_rate': 0.1,\n",
                "    'num_leaves': 127,\n",
                "    'min_child_samples': 50,\n",
                "    'subsample': 0.8,\n",
                "    'colsample_bytree': 0.8,\n",
                "    'reg_alpha': 0.1,\n",
                "    'reg_lambda': 1.0,\n",
                "    'verbosity': 1,\n",
                "    'seed': 42,\n",
                "    'n_jobs': -1\n",
                "}\n",
                "\n",
                "evals_result_lgb = {}\n",
                "callbacks = [\n",
                "    lgb.log_evaluation(period=50),\n",
                "    lgb.early_stopping(stopping_rounds=20),\n",
                "    lgb.record_evaluation(evals_result_lgb)\n",
                "]\n",
                "\n",
                "t0 = time.time()\n",
                "bst_lgb = lgb.train(\n",
                "    lgb_params,\n",
                "    lgb_train,\n",
                "    num_boost_round=300,\n",
                "    valid_sets=[lgb_train, lgb_test],\n",
                "    valid_names=['train', 'test'],\n",
                "    callbacks=callbacks\n",
                ")\n",
                "train_time_lgb = time.time() - t0\n",
                "\n",
                "t0 = time.time()\n",
                "y_prob_lgb = bst_lgb.predict(X_test, num_iteration=bst_lgb.best_iteration)\n",
                "y_pred_lgb = (y_prob_lgb > 0.5).astype(int)\n",
                "infer_time_lgb = time.time() - t0\n",
                "\n",
                "models['LightGBM GPU'] = bst_lgb\n",
                "predictions['LightGBM GPU'] = y_pred_lgb\n",
                "probabilities['LightGBM GPU'] = y_prob_lgb\n",
                "\n",
                "print(f\"\\n   üéÆ Trained on GPU\")\n",
                "print(f\"   ‚è±Ô∏è Train: {train_time_lgb:.1f}s | Inference: {infer_time_lgb:.1f}s\")\n",
                "print(f\"   üèÜ Best iteration: {bst_lgb.best_iteration}\")\n",
                "print(f\"   ‚úÖ Accuracy: {accuracy_score(y_test, y_pred_lgb)*100:.4f}%\")\n",
                "print(f\"   üéØ F1: {f1_score(y_test, y_pred_lgb)*100:.4f}%\")\n",
                "\n",
                "bst_lgb.save_model('models/binary_lgb_gpu.txt')\n",
                "print(\"   üíæ Saved to models/binary_lgb_gpu.txt\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Compare All Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build comparison table\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üìä BINARY CLASSIFICATION RESULTS ‚Äî ALL MODELS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "model_names = ['Decision Tree', 'Random Forest', 'XGBoost GPU', 'LightGBM GPU']\n",
                "train_times = [train_time_dt, train_time_rf, train_time_xgb, train_time_lgb]\n",
                "infer_times = [infer_time_dt, infer_time_rf, infer_time_xgb, infer_time_lgb]\n",
                "\n",
                "comparison_rows = []\n",
                "for name in model_names:\n",
                "    y_p = predictions[name]\n",
                "    y_pr = probabilities[name]\n",
                "    row = {\n",
                "        'Model': name,\n",
                "        'Accuracy': accuracy_score(y_test, y_p) * 100,\n",
                "        'F1': f1_score(y_test, y_p, average='binary') * 100,\n",
                "        'Precision': precision_score(y_test, y_p) * 100,\n",
                "        'Recall': recall_score(y_test, y_p) * 100,\n",
                "        'ROC-AUC': roc_auc_score(y_test, y_pr) * 100,\n",
                "        'Train Time (s)': train_times[model_names.index(name)],\n",
                "        'Infer Time (s)': infer_times[model_names.index(name)]\n",
                "    }\n",
                "    comparison_rows.append(row)\n",
                "\n",
                "comparison_df = pd.DataFrame(comparison_rows)\n",
                "comparison_df = comparison_df.set_index('Model')\n",
                "print(comparison_df.to_string(float_format=lambda x: f'{x:.4f}'))\n",
                "\n",
                "# Save results\n",
                "comparison_df.to_csv('models/binary_comparison.csv')\n",
                "print(\"\\nüíæ Saved to models/binary_comparison.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìä Visual Comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
                "\n",
                "# Metrics comparison\n",
                "metrics_plot = ['Accuracy', 'F1', 'Precision', 'Recall', 'ROC-AUC']\n",
                "x = np.arange(len(metrics_plot))\n",
                "width = 0.2\n",
                "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
                "\n",
                "for i, name in enumerate(model_names):\n",
                "    vals = [comparison_df.loc[name, m] for m in metrics_plot]\n",
                "    axes[0].bar(x + i*width, vals, width, label=name, color=colors[i], edgecolor='white')\n",
                "\n",
                "axes[0].set_xticks(x + width*1.5)\n",
                "axes[0].set_xticklabels(metrics_plot)\n",
                "axes[0].set_ylim(90, 101)\n",
                "axes[0].set_title('üìä Model Comparison ‚Äî Metrics (%)', fontsize=14, fontweight='bold', color='white')\n",
                "axes[0].legend(fontsize=10)\n",
                "axes[0].grid(True, alpha=0.2, axis='y')\n",
                "\n",
                "# Training time comparison\n",
                "axes[1].barh(model_names, train_times, color=colors, edgecolor='white', height=0.5)\n",
                "for i, t in enumerate(train_times):\n",
                "    axes[1].text(t + max(train_times)*0.02, i, f'{t:.1f}s', va='center', fontweight='bold', color='white')\n",
                "axes[1].set_title('‚è±Ô∏è Training Time (seconds)', fontsize=14, fontweight='bold', color='white')\n",
                "axes[1].grid(True, alpha=0.2, axis='x')\n",
                "\n",
                "plt.suptitle('Level 1: Binary Classification ‚Äî Model Comparison', fontsize=16, fontweight='bold', color='#00D4AA', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/binary_model_comparison.png', dpi=150, bbox_inches='tight', facecolor='#1a1a2e')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìä ROC Curves\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "\n",
                "for i, name in enumerate(model_names):\n",
                "    fpr, tpr, _ = roc_curve(y_test, probabilities[name])\n",
                "    roc_auc_val = auc(fpr, tpr)\n",
                "    ax.plot(fpr, tpr, color=colors[i], linewidth=2.5, label=f'{name} (AUC={roc_auc_val:.4f})')\n",
                "\n",
                "ax.plot([0, 1], [0, 1], 'w--', alpha=0.3, linewidth=1)\n",
                "ax.set_xlabel('False Positive Rate', fontsize=13)\n",
                "ax.set_ylabel('True Positive Rate', fontsize=13)\n",
                "ax.set_title('üìà ROC Curves ‚Äî Binary Classification', fontsize=16, fontweight='bold', color='#00D4AA')\n",
                "ax.legend(fontsize=12, loc='lower right')\n",
                "ax.grid(True, alpha=0.2)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/binary_roc_curves.png', dpi=150, bbox_inches='tight', facecolor='#1a1a2e')\n",
                "plt.show()\n",
                "print(\"üíæ Saved to figures/binary_roc_curves.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìä Confusion Matrices for all 4 models\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
                "\n",
                "for ax, name in zip(axes.ravel(), model_names):\n",
                "    cm = confusion_matrix(y_test, predictions[name])\n",
                "    cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
                "    sns.heatmap(cm_pct, annot=True, fmt='.2f', cmap='RdYlGn',\n",
                "                xticklabels=['Benign', 'Attack'], yticklabels=['Benign', 'Attack'],\n",
                "                ax=ax, linewidths=2, linecolor='white',\n",
                "                annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
                "    ax.set_title(f'{name}', fontsize=14, fontweight='bold', color='white')\n",
                "    ax.set_xlabel('Predicted', fontsize=11)\n",
                "    ax.set_ylabel('Actual', fontsize=11)\n",
                "\n",
                "plt.suptitle('üìä Confusion Matrices ‚Äî Binary Classification (% per class)', fontsize=16, fontweight='bold', color='#00D4AA', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/binary_confusion_matrices.png', dpi=150, bbox_inches='tight', facecolor='#1a1a2e')\n",
                "plt.show()\n",
                "print(\"üíæ Saved to figures/binary_confusion_matrices.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification Reports\n",
                "for name in model_names:\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"üìã {name} ‚Äî Classification Report\")\n",
                "    print(f\"{'='*60}\")\n",
                "    print(classification_report(y_test, predictions[name], target_names=['Benign', 'Attack']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save all results\n",
                "binary_results = {\n",
                "    'timestamp': datetime.now().isoformat(),\n",
                "    'level': 'Binary (Benign vs Attack)',\n",
                "    'device': 'GPU (CUDA) for XGBoost & LightGBM',\n",
                "    'models': {}\n",
                "}\n",
                "for name in model_names:\n",
                "    y_p = predictions[name]\n",
                "    y_pr = probabilities[name]\n",
                "    cm = confusion_matrix(y_test, y_p)\n",
                "    binary_results['models'][name] = {\n",
                "        'accuracy': float(accuracy_score(y_test, y_p)),\n",
                "        'f1': float(f1_score(y_test, y_p)),\n",
                "        'precision': float(precision_score(y_test, y_p)),\n",
                "        'recall': float(recall_score(y_test, y_p)),\n",
                "        'roc_auc': float(roc_auc_score(y_test, y_pr)),\n",
                "        'train_time': train_times[model_names.index(name)],\n",
                "        'inference_time': infer_times[model_names.index(name)],\n",
                "        'confusion_matrix': cm.tolist()\n",
                "    }\n",
                "\n",
                "with open('models/binary_results.json', 'w') as f:\n",
                "    json.dump(binary_results, f, indent=2)\n",
                "\n",
                "print(\"\\nüèÜ\" * 20)\n",
                "print(f\"\\n  ‚úÖ BINARY CLASSIFICATION COMPLETE!\")\n",
                "print(f\"  üìä 4 models trained and evaluated\")\n",
                "print(f\"  üéÆ XGBoost & LightGBM on GPU\")\n",
                "print(f\"  üíæ All results saved\")\n",
                "print(f\"\\n\" + \"üèÜ\" * 20)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}