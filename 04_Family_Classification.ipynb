{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìÖ Day 4: Level 2 ‚Äî 8-Class Family Classification\n",
                "## DDoS, DoS, Mirai, Recon, Spoofing, Web, BruteForce, Benign\n",
                "\n",
                "---\n",
                "\n",
                "**Steps:**\n",
                "1. Load preprocessed data\n",
                "2. Round 1: Train with class weights only\n",
                "3. Round 2: Apply SMOTE for Web + BruteForce\n",
                "4. Round 3: Undersample DDoS + SMOTE minorities\n",
                "5. Hyperparameter Tuning with Optuna (best model)\n",
                "6. Compare all rounds\n",
                "7. Per-class analysis + Confusion Matrix (8√ó8)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.add_dll_directory(r'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v13.1\\bin\\x64')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import xgboost as xgb\n",
                "import lightgbm as lgb\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,\n",
                "                             classification_report, confusion_matrix)\n",
                "from sklearn.utils.class_weight import compute_sample_weight\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.under_sampling import RandomUnderSampler\n",
                "from imblearn.pipeline import Pipeline as ImbPipeline\n",
                "import optuna\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import time\n",
                "import gc\n",
                "import json\n",
                "import joblib\n",
                "from datetime import datetime\n",
                "\n",
                "plt.style.use('dark_background')\n",
                "plt.rcParams['figure.figsize'] = (14, 6)\n",
                "plt.rcParams['font.size'] = 12\n",
                "\n",
                "os.makedirs('models', exist_ok=True)\n",
                "os.makedirs('figures', exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Ready | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì• Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üì• Loading preprocessed data...\")\n",
                "t0 = time.time()\n",
                "\n",
                "X_train = np.load('processed/X_train.npy')\n",
                "X_test = np.load('processed/X_test.npy')\n",
                "y_train = np.load('processed/y_family_train.npy')\n",
                "y_test = np.load('processed/y_family_test.npy')\n",
                "\n",
                "with open('processed/preprocessing_metadata.json', 'r') as f:\n",
                "    meta = json.load(f)\n",
                "feature_names = meta['feature_names']\n",
                "family_classes = meta['family_classes']\n",
                "n_classes = len(family_classes)\n",
                "\n",
                "print(f\"‚úÖ Loaded in {time.time()-t0:.1f}s\")\n",
                "print(f\"   X_train: {X_train.shape} | X_test: {X_test.shape}\")\n",
                "print(f\"   Classes ({n_classes}): {family_classes}\")\n",
                "\n",
                "# Class distribution\n",
                "print(f\"\\nüìä Training class distribution:\")\n",
                "for i, name in enumerate(family_classes):\n",
                "    count = (y_train == i).sum()\n",
                "    print(f\"   {i}: {name:<20s} ‚Üí {count:>10,} ({count/len(y_train)*100:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèãÔ∏è Round 1: Class Weights Only (No Data Manipulation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round1_results = {}\n",
                "round1_predictions = {}\n",
                "\n",
                "# Compute sample weights for XGBoost\n",
                "sample_weights = compute_sample_weight('balanced', y_train)\n",
                "\n",
                "model_configs = {\n",
                "    'Decision Tree': {\n",
                "        'model': DecisionTreeClassifier(class_weight='balanced', max_depth=15, random_state=42)\n",
                "    },\n",
                "    'Random Forest': {\n",
                "        'model': RandomForestClassifier(n_estimators=200, class_weight='balanced', max_depth=15, n_jobs=-1, random_state=42)\n",
                "    }\n",
                "}\n",
                "\n",
                "# Train sklearn models\n",
                "for name, cfg in model_configs.items():\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"üèãÔ∏è Round 1 ‚Äî {name}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    t0 = time.time()\n",
                "    cfg['model'].fit(X_train, y_train)\n",
                "    train_time = time.time() - t0\n",
                "    \n",
                "    y_pred = cfg['model'].predict(X_test)\n",
                "    \n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    f1_mac = f1_score(y_test, y_pred, average='macro')\n",
                "    f1_wtd = f1_score(y_test, y_pred, average='weighted')\n",
                "    \n",
                "    round1_results[name] = {'accuracy': acc, 'f1_macro': f1_mac, 'f1_weighted': f1_wtd, 'train_time': train_time}\n",
                "    round1_predictions[name] = y_pred\n",
                "    \n",
                "    print(f\"   ‚è±Ô∏è Train: {train_time:.1f}s\")\n",
                "    print(f\"   ‚úÖ Accuracy: {acc*100:.4f}% | F1-Macro: {f1_mac*100:.4f}% | F1-Weighted: {f1_wtd*100:.4f}%\")\n",
                "    \n",
                "    joblib.dump(cfg['model'], f'models/family_r1_{name.lower().replace(\" \",\"_\")}.joblib')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# XGBoost GPU ‚Äî Round 1\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"üéÆ Round 1 ‚Äî XGBoost GPU\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "dtrain = xgb.DMatrix(X_train, label=y_train, weight=sample_weights, feature_names=feature_names)\n",
                "dtest_xgb = xgb.DMatrix(X_test, label=y_test, feature_names=feature_names)\n",
                "\n",
                "xgb_params = {\n",
                "    'tree_method': 'hist',\n",
                "    'device': 'cuda',\n",
                "    'objective': 'multi:softprob',\n",
                "    'num_class': n_classes,\n",
                "    'eval_metric': ['mlogloss', 'merror'],\n",
                "    'max_depth': 8,\n",
                "    'learning_rate': 0.1,\n",
                "    'min_child_weight': 5,\n",
                "    'subsample': 0.8,\n",
                "    'colsample_bytree': 0.8,\n",
                "    'reg_alpha': 0.1,\n",
                "    'reg_lambda': 1.0,\n",
                "    'verbosity': 1,\n",
                "    'seed': 42\n",
                "}\n",
                "\n",
                "evals_result_xgb = {}\n",
                "t0 = time.time()\n",
                "bst_xgb = xgb.train(\n",
                "    xgb_params, dtrain,\n",
                "    num_boost_round=300,\n",
                "    evals=[(dtrain, 'train'), (dtest_xgb, 'test')],\n",
                "    early_stopping_rounds=20,\n",
                "    evals_result=evals_result_xgb,\n",
                "    verbose_eval=50\n",
                ")\n",
                "train_time_xgb = time.time() - t0\n",
                "\n",
                "y_prob_xgb = bst_xgb.predict(dtest_xgb, iteration_range=(0, bst_xgb.best_iteration + 1))\n",
                "y_pred_xgb = y_prob_xgb.argmax(axis=1)\n",
                "\n",
                "acc = accuracy_score(y_test, y_pred_xgb)\n",
                "f1_mac = f1_score(y_test, y_pred_xgb, average='macro')\n",
                "f1_wtd = f1_score(y_test, y_pred_xgb, average='weighted')\n",
                "\n",
                "round1_results['XGBoost GPU'] = {'accuracy': acc, 'f1_macro': f1_mac, 'f1_weighted': f1_wtd, 'train_time': train_time_xgb}\n",
                "round1_predictions['XGBoost GPU'] = y_pred_xgb\n",
                "\n",
                "print(f\"\\n   üéÆ GPU Training | ‚è±Ô∏è {train_time_xgb:.1f}s | Best iter: {bst_xgb.best_iteration}\")\n",
                "print(f\"   ‚úÖ Accuracy: {acc*100:.4f}% | F1-Macro: {f1_mac*100:.4f}% | F1-Weighted: {f1_wtd*100:.4f}%\")\n",
                "\n",
                "bst_xgb.save_model('models/family_r1_xgb_gpu.json')\n",
                "del dtrain; gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LightGBM GPU ‚Äî Round 1\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"üéÆ Round 1 ‚Äî LightGBM GPU\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "lgb_params = {\n",
                "    'objective': 'multiclass',\n",
                "    'num_class': n_classes,\n",
                "    'metric': ['multi_logloss', 'multi_error'],\n",
                "    'device': 'gpu',\n",
                "    'gpu_use_dp': False,\n",
                "    'class_weight': 'balanced',\n",
                "    'max_depth': 8,\n",
                "    'learning_rate': 0.1,\n",
                "    'num_leaves': 127,\n",
                "    'min_child_samples': 50,\n",
                "    'subsample': 0.8,\n",
                "    'colsample_bytree': 0.8,\n",
                "    'reg_alpha': 0.1,\n",
                "    'reg_lambda': 1.0,\n",
                "    'verbosity': 1,\n",
                "    'seed': 42,\n",
                "    'n_jobs': -1\n",
                "}\n",
                "\n",
                "lgb_train = lgb.Dataset(X_train, label=y_train, feature_name=feature_names, free_raw_data=False)\n",
                "lgb_test = lgb.Dataset(X_test, label=y_test, feature_name=feature_names, reference=lgb_train, free_raw_data=False)\n",
                "\n",
                "evals_result_lgb = {}\n",
                "t0 = time.time()\n",
                "bst_lgb = lgb.train(\n",
                "    lgb_params, lgb_train,\n",
                "    num_boost_round=300,\n",
                "    valid_sets=[lgb_train, lgb_test],\n",
                "    valid_names=['train', 'test'],\n",
                "    callbacks=[\n",
                "        lgb.log_evaluation(50),\n",
                "        lgb.early_stopping(20),\n",
                "        lgb.record_evaluation(evals_result_lgb)\n",
                "    ]\n",
                ")\n",
                "train_time_lgb = time.time() - t0\n",
                "\n",
                "y_prob_lgb = bst_lgb.predict(X_test, num_iteration=bst_lgb.best_iteration)\n",
                "y_pred_lgb = y_prob_lgb.argmax(axis=1)\n",
                "\n",
                "acc = accuracy_score(y_test, y_pred_lgb)\n",
                "f1_mac = f1_score(y_test, y_pred_lgb, average='macro')\n",
                "f1_wtd = f1_score(y_test, y_pred_lgb, average='weighted')\n",
                "\n",
                "round1_results['LightGBM GPU'] = {'accuracy': acc, 'f1_macro': f1_mac, 'f1_weighted': f1_wtd, 'train_time': train_time_lgb}\n",
                "round1_predictions['LightGBM GPU'] = y_pred_lgb\n",
                "\n",
                "print(f\"\\n   üéÆ GPU Training | ‚è±Ô∏è {train_time_lgb:.1f}s | Best iter: {bst_lgb.best_iteration}\")\n",
                "print(f\"   ‚úÖ Accuracy: {acc*100:.4f}% | F1-Macro: {f1_mac*100:.4f}% | F1-Weighted: {f1_wtd*100:.4f}%\")\n",
                "\n",
                "bst_lgb.save_model('models/family_r1_lgb_gpu.txt')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Round 1 Summary\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üìä ROUND 1 RESULTS ‚Äî Class Weights Only\")\n",
                "print(\"=\"*80)\n",
                "r1_df = pd.DataFrame(round1_results).T\n",
                "r1_df.columns = ['Accuracy', 'F1-Macro', 'F1-Weighted', 'Train Time']\n",
                "r1_df[['Accuracy', 'F1-Macro', 'F1-Weighted']] *= 100\n",
                "print(r1_df.to_string(float_format=lambda x: f'{x:.4f}'))\n",
                "\n",
                "# Identify best model\n",
                "best_model_name = r1_df['F1-Macro'].idxmax()\n",
                "print(f\"\\nüèÜ Best model (F1-Macro): {best_model_name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß™ Round 2: SMOTE for Web + BruteForce"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply SMOTE to oversample minority classes\n",
                "print(\"üß™ Round 2: Applying SMOTE for minority classes...\")\n",
                "t0 = time.time()\n",
                "\n",
                "# Find minority classes' indices\n",
                "class_counts = pd.Series(y_train).value_counts()\n",
                "print(f\"   Before SMOTE: {dict(class_counts)}\")\n",
                "\n",
                "# Set target: bring minorities up to at least median count\n",
                "median_count = int(class_counts.median())\n",
                "sampling_strategy = {}\n",
                "for cls_id, count in class_counts.items():\n",
                "    if count < median_count:\n",
                "        sampling_strategy[cls_id] = min(median_count, count * 10)  # Cap at 10x\n",
                "\n",
                "print(f\"   SMOTE targets: {sampling_strategy}\")\n",
                "\n",
                "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42, n_jobs=-1)\n",
                "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
                "\n",
                "print(f\"   After SMOTE: {X_train_smote.shape[0]:,} samples (was {X_train.shape[0]:,})\")\n",
                "print(f\"   ‚è±Ô∏è SMOTE done in {time.time()-t0:.1f}s\")\n",
                "\n",
                "# Show new distribution\n",
                "print(f\"\\n   New distribution:\")\n",
                "for i, name in enumerate(family_classes):\n",
                "    count = (y_train_smote == i).sum()\n",
                "    print(f\"   {i}: {name:<20s} ‚Üí {count:>10,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train best model with SMOTE data on GPU\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"üéÆ Round 2 ‚Äî XGBoost GPU (SMOTE data)\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "dtrain_smote = xgb.DMatrix(X_train_smote, label=y_train_smote, feature_names=feature_names)\n",
                "\n",
                "t0 = time.time()\n",
                "bst_xgb_r2 = xgb.train(\n",
                "    xgb_params, dtrain_smote,\n",
                "    num_boost_round=300,\n",
                "    evals=[(dtrain_smote, 'train'), (dtest_xgb, 'test')],\n",
                "    early_stopping_rounds=20,\n",
                "    verbose_eval=50\n",
                ")\n",
                "train_time_r2 = time.time() - t0\n",
                "\n",
                "y_pred_r2 = bst_xgb_r2.predict(dtest_xgb, iteration_range=(0, bst_xgb_r2.best_iteration + 1)).argmax(axis=1)\n",
                "\n",
                "acc_r2 = accuracy_score(y_test, y_pred_r2)\n",
                "f1_mac_r2 = f1_score(y_test, y_pred_r2, average='macro')\n",
                "f1_wtd_r2 = f1_score(y_test, y_pred_r2, average='weighted')\n",
                "\n",
                "print(f\"\\n   üéÆ GPU Training | ‚è±Ô∏è {train_time_r2:.1f}s\")\n",
                "print(f\"   ‚úÖ Accuracy: {acc_r2*100:.4f}% | F1-Macro: {f1_mac_r2*100:.4f}% | F1-Weighted: {f1_wtd_r2*100:.4f}%\")\n",
                "\n",
                "bst_xgb_r2.save_model('models/family_r2_xgb_gpu.json')\n",
                "del dtrain_smote, X_train_smote, y_train_smote; gc.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß™ Round 3: Undersample Majority + SMOTE Minorities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üß™ Round 3: Undersample majority + SMOTE minorities...\")\n",
                "t0 = time.time()\n",
                "\n",
                "class_counts = pd.Series(y_train).value_counts()\n",
                "\n",
                "# Undersample: cap majority classes at 30K\n",
                "under_strategy = {}\n",
                "for cls_id, count in class_counts.items():\n",
                "    if count > 30000:\n",
                "        under_strategy[cls_id] = 30000\n",
                "\n",
                "# SMOTE: bring minorities up to 5000\n",
                "over_strategy = {}\n",
                "for cls_id, count in class_counts.items():\n",
                "    effective_count = min(count, under_strategy.get(cls_id, count))\n",
                "    if effective_count < 5000:\n",
                "        over_strategy[cls_id] = 5000\n",
                "\n",
                "print(f\"   Under-sampling: {under_strategy}\")\n",
                "print(f\"   Over-sampling: {over_strategy}\")\n",
                "\n",
                "pipeline = ImbPipeline([\n",
                "    ('under', RandomUnderSampler(sampling_strategy=under_strategy, random_state=42)),\n",
                "    ('over', SMOTE(sampling_strategy=over_strategy, random_state=42, n_jobs=-1))\n",
                "])\n",
                "\n",
                "X_train_r3, y_train_r3 = pipeline.fit_resample(X_train, y_train)\n",
                "print(f\"   Result: {X_train_r3.shape[0]:,} samples\")\n",
                "print(f\"   ‚è±Ô∏è Done in {time.time()-t0:.1f}s\")\n",
                "\n",
                "for i, name in enumerate(family_classes):\n",
                "    count = (y_train_r3 == i).sum()\n",
                "    print(f\"   {i}: {name:<20s} ‚Üí {count:>10,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train XGBoost GPU on Round 3 data\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"üéÆ Round 3 ‚Äî XGBoost GPU (Under+SMOTE data)\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "dtrain_r3 = xgb.DMatrix(X_train_r3, label=y_train_r3, feature_names=feature_names)\n",
                "\n",
                "t0 = time.time()\n",
                "bst_xgb_r3 = xgb.train(\n",
                "    xgb_params, dtrain_r3,\n",
                "    num_boost_round=300,\n",
                "    evals=[(dtrain_r3, 'train'), (dtest_xgb, 'test')],\n",
                "    early_stopping_rounds=20,\n",
                "    verbose_eval=50\n",
                ")\n",
                "train_time_r3 = time.time() - t0\n",
                "\n",
                "y_pred_r3 = bst_xgb_r3.predict(dtest_xgb, iteration_range=(0, bst_xgb_r3.best_iteration + 1)).argmax(axis=1)\n",
                "\n",
                "acc_r3 = accuracy_score(y_test, y_pred_r3)\n",
                "f1_mac_r3 = f1_score(y_test, y_pred_r3, average='macro')\n",
                "f1_wtd_r3 = f1_score(y_test, y_pred_r3, average='weighted')\n",
                "\n",
                "print(f\"\\n   üéÆ GPU Training | ‚è±Ô∏è {train_time_r3:.1f}s\")\n",
                "print(f\"   ‚úÖ Accuracy: {acc_r3*100:.4f}% | F1-Macro: {f1_mac_r3*100:.4f}% | F1-Weighted: {f1_wtd_r3*100:.4f}%\")\n",
                "\n",
                "bst_xgb_r3.save_model('models/family_r3_xgb_gpu.json')\n",
                "del dtrain_r3, X_train_r3, y_train_r3; gc.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß Hyperparameter Tuning with Optuna (Best Model ‚Äî XGBoost GPU)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîß Optuna Hyperparameter Tuning ‚Äî XGBoost GPU\")\n",
                "print(\"   Optimizing: Macro F1-Score\")\n",
                "print(\"   Trials: 30\")\n",
                "\n",
                "dtrain_opt = xgb.DMatrix(X_train, label=y_train, weight=sample_weights, feature_names=feature_names)\n",
                "\n",
                "def objective(trial):\n",
                "    params = {\n",
                "        'tree_method': 'hist',\n",
                "        'device': 'cuda',\n",
                "        'objective': 'multi:softprob',\n",
                "        'num_class': n_classes,\n",
                "        'eval_metric': 'mlogloss',\n",
                "        'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
                "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
                "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
                "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
                "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
                "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
                "        'verbosity': 0,\n",
                "        'seed': 42\n",
                "    }\n",
                "    \n",
                "    n_rounds = trial.suggest_int('n_estimators', 100, 500)\n",
                "    \n",
                "    bst = xgb.train(\n",
                "        params, dtrain_opt,\n",
                "        num_boost_round=n_rounds,\n",
                "        evals=[(dtest_xgb, 'test')],\n",
                "        early_stopping_rounds=20,\n",
                "        verbose_eval=False\n",
                "    )\n",
                "    \n",
                "    y_prob = bst.predict(dtest_xgb, iteration_range=(0, bst.best_iteration + 1))\n",
                "    y_pred = y_prob.argmax(axis=1)\n",
                "    f1_mac = f1_score(y_test, y_pred, average='macro')\n",
                "    \n",
                "    return f1_mac\n",
                "\n",
                "study = optuna.create_study(direction='maximize', study_name='family_xgb_gpu')\n",
                "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
                "\n",
                "print(f\"\\nüèÜ Best trial:\")\n",
                "print(f\"   F1-Macro: {study.best_value*100:.4f}%\")\n",
                "print(f\"   Params: {study.best_params}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train final model with best params\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"üéÆ Training Final Model with Best Params (GPU)\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "best_params = study.best_params.copy()\n",
                "n_rounds_best = best_params.pop('n_estimators')\n",
                "best_params.update({\n",
                "    'tree_method': 'hist',\n",
                "    'device': 'cuda',\n",
                "    'objective': 'multi:softprob',\n",
                "    'num_class': n_classes,\n",
                "    'eval_metric': ['mlogloss', 'merror'],\n",
                "    'verbosity': 1,\n",
                "    'seed': 42\n",
                "})\n",
                "\n",
                "evals_result_best = {}\n",
                "t0 = time.time()\n",
                "bst_best = xgb.train(\n",
                "    best_params, dtrain_opt,\n",
                "    num_boost_round=n_rounds_best,\n",
                "    evals=[(dtrain_opt, 'train'), (dtest_xgb, 'test')],\n",
                "    early_stopping_rounds=20,\n",
                "    evals_result=evals_result_best,\n",
                "    verbose_eval=50\n",
                ")\n",
                "train_time_best = time.time() - t0\n",
                "\n",
                "y_pred_best = bst_best.predict(dtest_xgb, iteration_range=(0, bst_best.best_iteration + 1)).argmax(axis=1)\n",
                "\n",
                "acc_best = accuracy_score(y_test, y_pred_best)\n",
                "f1_mac_best = f1_score(y_test, y_pred_best, average='macro')\n",
                "f1_wtd_best = f1_score(y_test, y_pred_best, average='weighted')\n",
                "\n",
                "print(f\"\\n   üéÆ GPU | ‚è±Ô∏è {train_time_best:.1f}s | Best iter: {bst_best.best_iteration}\")\n",
                "print(f\"   ‚úÖ Accuracy: {acc_best*100:.4f}% | F1-Macro: {f1_mac_best*100:.4f}% | F1-Weighted: {f1_wtd_best*100:.4f}%\")\n",
                "\n",
                "bst_best.save_model('models/family_best_xgb_gpu.json')\n",
                "print(\"   üíæ Saved to models/family_best_xgb_gpu.json\")\n",
                "del dtrain_opt; gc.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Evaluation & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Per-class classification report for best model\n",
                "print(\"=\"*60)\n",
                "print(\"üìã Best Model ‚Äî Per-Class Classification Report\")\n",
                "print(\"=\"*60)\n",
                "print(classification_report(y_test, y_pred_best, target_names=family_classes, digits=4))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìä 8x8 Confusion Matrix Heatmap\n",
                "cm = confusion_matrix(y_test, y_pred_best)\n",
                "cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(22, 8))\n",
                "\n",
                "sns.heatmap(cm, annot=True, fmt=',d', cmap='YlOrRd', xticklabels=family_classes, yticklabels=family_classes,\n",
                "            ax=axes[0], linewidths=1, linecolor='white', annot_kws={'fontsize': 9})\n",
                "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold', color='white')\n",
                "axes[0].set_xlabel('Predicted', fontsize=12)\n",
                "axes[0].set_ylabel('Actual', fontsize=12)\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='RdYlGn', xticklabels=family_classes, yticklabels=family_classes,\n",
                "            ax=axes[1], linewidths=1, linecolor='white', annot_kws={'fontsize': 9})\n",
                "axes[1].set_title('Confusion Matrix (% per class)', fontsize=14, fontweight='bold', color='white')\n",
                "axes[1].set_xlabel('Predicted', fontsize=12)\n",
                "axes[1].set_ylabel('Actual', fontsize=12)\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "plt.suptitle('üìä 8-Class Family Classification ‚Äî Confusion Matrix', fontsize=16, fontweight='bold', color='#00D4AA', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/family_confusion_matrix.png', dpi=150, bbox_inches='tight', facecolor='#1a1a2e')\n",
                "plt.show()\n",
                "print(\"üíæ Saved to figures/family_confusion_matrix.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìä Round comparison\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üìä ALL ROUNDS COMPARISON\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "rounds_df = pd.DataFrame({\n",
                "    'Round': ['R1: Class Weights', 'R2: SMOTE', 'R3: Under+SMOTE', 'Optuna Tuned'],\n",
                "    'F1-Macro (%)': [\n",
                "        round1_results['XGBoost GPU']['f1_macro']*100,\n",
                "        f1_mac_r2*100,\n",
                "        f1_mac_r3*100,\n",
                "        f1_mac_best*100\n",
                "    ],\n",
                "    'Accuracy (%)': [\n",
                "        round1_results['XGBoost GPU']['accuracy']*100,\n",
                "        acc_r2*100,\n",
                "        acc_r3*100,\n",
                "        acc_best*100\n",
                "    ]\n",
                "})\n",
                "print(rounds_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
                "\n",
                "# Save all results\n",
                "family_results = {\n",
                "    'timestamp': datetime.now().isoformat(),\n",
                "    'level': '8-Class Family',\n",
                "    'device': 'GPU (CUDA)',\n",
                "    'round1': round1_results,\n",
                "    'round2_smote': {'accuracy': acc_r2, 'f1_macro': f1_mac_r2, 'f1_weighted': f1_wtd_r2},\n",
                "    'round3_under_smote': {'accuracy': acc_r3, 'f1_macro': f1_mac_r3, 'f1_weighted': f1_wtd_r3},\n",
                "    'optuna_best': {\n",
                "        'accuracy': acc_best, 'f1_macro': f1_mac_best, 'f1_weighted': f1_wtd_best,\n",
                "        'params': study.best_params\n",
                "    }\n",
                "}\n",
                "with open('models/family_results.json', 'w') as f:\n",
                "    json.dump(family_results, f, indent=2, default=str)\n",
                "\n",
                "print(\"\\nüèÜ\" * 20)\n",
                "print(f\"  ‚úÖ FAMILY CLASSIFICATION COMPLETE!\")\n",
                "print(f\"  üéÆ All GPU-accelerated\")\n",
                "print(\"üèÜ\" * 20)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}